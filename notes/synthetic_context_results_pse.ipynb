{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"../src\" not in sys.path:\n",
    "    sys.path.insert(0, \"../src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import copy\n",
    "import pprint\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import bisect\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import read_json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mimer/NOBACKUP/groups/snic2022-22-1003/APP/rag-memory-interplay/venv/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from experiments import nethook\n",
    "from experiments.tools import make_inputs\n",
    "from experiments.utils import load_atlas\n",
    "from experiments.dataset import KnownsDataset\n",
    "from experiments.tools import (\n",
    "    collect_embedding_std,\n",
    "    calculate_hidden_flow,\n",
    "    # plot_trace_heatmap,\n",
    "    predict_token,\n",
    "    find_token_ranges,\n",
    "    prompt_segmenter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mimer/NOBACKUP/groups/snic2022-22-1003/APP/rag-memory-interplay/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/contriever were not used when initializing Contriever: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing Contriever from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Contriever from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "atlas.atlas.Atlas"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_CACHE_DIR\"] = f\"../../caches/wandb\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"]= f\"../../.cache/huggingface/transformers\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = f\"../../.cache/huggingface/datasets\"\n",
    "\n",
    "SIZE = \"base\"  # \"base\", \"large\"\n",
    "QA_PROMPT_FORMAT = \"question: {question} answer: <extra_id_0>\"\n",
    "\n",
    "reader_model_type = f\"google/t5-{SIZE}-lm-adapt\"\n",
    "model_path = f\"../data/atlas/models/atlas_nq/{SIZE}\"\n",
    "model, opt = load_atlas(reader_model_type, model_path, n_context=1, qa_prompt_format=\"question: {question} answer: <extra_id_0>\")\n",
    "type(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_division(a, b, do_log=False, threshold=None):\n",
    "    threshold = threshold if threshold else np.finfo(np.float32).max/10.0\n",
    "    return torch.clamp(a/b, max=threshold) if not do_log else (torch.log(a) - torch.log(b))\n",
    "\n",
    "\n",
    "def calculate_te_ie(r, samples, experiment_type=\"a\", do_log=True, threshold=1e-40):\n",
    "    cr_cf_score = torch.clamp(r[\"cr_cf_score\"], min=threshold)\n",
    "    cr_ans_score = torch.clamp(r[\"cr_ans_score\"], min=threshold)\n",
    "\n",
    "    crr_cf_score = torch.clamp(r[\"crr_cf_score\"], min=threshold)\n",
    "    crr_ans_score = torch.clamp(r[\"crr_ans_score\"], min=threshold)\n",
    "\n",
    "    crwrr_cf_score = torch.clamp(r[\"crwrr_cf_score\"], min=threshold)\n",
    "    crwrr_ans_score = torch.clamp(r[\"crwrr_ans_score\"], min=threshold)\n",
    "    \n",
    "\n",
    "    te, ie = [], []\n",
    "\n",
    "    for i in range(samples):\n",
    "        if experiment_type == \"a\" or experiment_type == \"aa\":\n",
    "            te_i = safe_division(crr_cf_score[i], crr_ans_score[i], do_log=do_log) - \\\n",
    "                   safe_division(cr_cf_score[i], cr_ans_score, do_log=do_log)\n",
    "            ie_i = safe_division(crwrr_cf_score[:, :, i], crwrr_ans_score[:, :, i], do_log=do_log) - \\\n",
    "                   safe_division(cr_cf_score[i], cr_ans_score, do_log=do_log)\n",
    "        else:\n",
    "            te_i = safe_division(cr_cf_score[i], cr_ans_score, do_log=do_log) - \\\n",
    "                   safe_division(crr_cf_score[i], crr_ans_score[i], do_log=do_log)\n",
    "            ie_i = safe_division(crwrr_cf_score[:, :, i], crwrr_ans_score[:, :, i], do_log=do_log) - \\\n",
    "                   safe_division(crr_cf_score[i], crr_ans_score[i], do_log=do_log)\n",
    "\n",
    "                \n",
    "\n",
    "        te.append(te_i)\n",
    "        ie.append(ie_i.unsqueeze(-1))\n",
    "\n",
    "    te = torch.stack(te).mean()\n",
    "    ie = torch.cat(ie, axis=-1).mean(-1)\n",
    "\n",
    "    return te, ie\n",
    "\n",
    "\n",
    "def calculate_post_proc(r, samples, experiment_type=\"a\"):\n",
    "    r = {k: torch.from_numpy(v) if isinstance(v, np.ndarray) else v for k, v in r.items()}\n",
    "    \n",
    "    if r[\"status\"]:\n",
    "        te_log, ie_log = calculate_te_ie(r, samples=samples, experiment_type=experiment_type, do_log=True)\n",
    "        r[\"te_log\"] = te_log\n",
    "        r[\"ie_log\"] = ie_log\n",
    "\n",
    "        te, ie = calculate_te_ie(r, samples=samples, experiment_type=experiment_type, do_log=False)\n",
    "        r[\"te\"] = te\n",
    "        r[\"ie\"] = ie\n",
    "    else:\n",
    "        r[\"te_log\"] = None\n",
    "        r[\"ie_log\"] = None\n",
    "        r[\"te\"] = None\n",
    "        r[\"ie\"] = None\n",
    "\n",
    "    r = {k: v.detach().cpu().numpy() if torch.is_tensor(v) else v for k, v in r.items()}\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Avg:\n",
    "    def __init__(self, size=12, name=None):\n",
    "        self.d = []\n",
    "        self.size = size\n",
    "        self.name = name\n",
    "\n",
    "    def add(self, v):\n",
    "        if v.size > 0:\n",
    "            self.d.append(v[None])\n",
    "\n",
    "    def add_all(self, vv):\n",
    "        if vv.size > 0:\n",
    "            self.d.append(vv)\n",
    "\n",
    "    def avg(self):\n",
    "        if len(self.d) > 0:\n",
    "            non_empty_arrays = [arr for arr in self.d if arr.size > 0]\n",
    "            \n",
    "            if len(non_empty_arrays) > 0:\n",
    "                return np.concatenate(non_empty_arrays).mean(axis=0)\n",
    "            else:\n",
    "                return np.zeros(self.size)\n",
    "\n",
    "        return np.zeros(self.size)\n",
    "\n",
    "    def std(self):\n",
    "        if len(self.d) > 0:\n",
    "            non_empty_arrays = [arr for arr in self.d if arr.size > 0]\n",
    "            \n",
    "            if len(non_empty_arrays) > 0:\n",
    "                return np.concatenate(non_empty_arrays).std(axis=0)\n",
    "            else:\n",
    "                return np.zeros(self.size)\n",
    "\n",
    "        return np.zeros(self.size)\n",
    "\n",
    "    def size(self):\n",
    "        return sum(datum.shape[0] for datum in self.d)\n",
    "\n",
    "    def humanize(self):\n",
    "        return self.name.replace(\"_\", \" \").capitalize()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Avg name={self.name}>\"\n",
    "\n",
    "\n",
    "def plot_array(\n",
    "    differences,\n",
    "    ax,\n",
    "    labels,\n",
    "    kind=None,\n",
    "    savepdf=None,\n",
    "    title=None,\n",
    "    low_score=None,\n",
    "    high_score=None,\n",
    "    archname=\"Atlas\",\n",
    "    show_y_labels=True\n",
    "):\n",
    "    if low_score is None:\n",
    "        low_score = differences.min()\n",
    "    if high_score is None:\n",
    "        high_score = differences.max()\n",
    "        \n",
    "    answer = \"AIE\"\n",
    "    labels = labels\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(3.5, 2), dpi=300)\n",
    "\n",
    "    h = ax.pcolor(\n",
    "        differences,\n",
    "        cmap={None: \"Purples\", \"mlp\": \"Greens\", \"attn\": \"Reds\"}[kind],\n",
    "        vmin=low_score,\n",
    "        vmax=high_score,\n",
    "    )\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "        \n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticks([0.5 + i for i in range(len(differences))])\n",
    "    # ax.set_xticks([0.5 + i for i in range(0, differences.shape[1] - 6, 5)])\n",
    "    # ax.set_xticklabels(list(range(0, differences.shape[1] - 6, 5)))\n",
    "    ax.set_xticks([0.5 + i for i in range(0, differences.shape[1])])\n",
    "    ax.set_xticklabels(list(range(0, differences.shape[1])))\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    if show_y_labels:\n",
    "        ax.set_yticklabels(labels)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    if kind is None:\n",
    "        ax.set_xlabel(f\"single patched layer\")\n",
    "        # ax.set_xlabel(f\"single patched layer within {archname}\")\n",
    "    else:\n",
    "        ax.set_xlabel(f\"center of interval of 6 patched {kind} layers\")\n",
    "        # ax.set_xlabel(f\"center of interval of 6 patched {kind} layers within {archname}\")\n",
    "\n",
    "    cb = plt.colorbar(h)\n",
    "    # The following should be cb.ax.set_xlabel(answer), but this is broken in matplotlib 3.5.1.\n",
    "    if answer:\n",
    "        # cb.ax.set_title(str(answer).strip(), y=-0.16, fontsize=10)\n",
    "        cb.ax.set_title(str(answer).strip(), y=-0.16)\n",
    "\n",
    "    if savepdf:\n",
    "        os.makedirs(os.path.dirname(savepdf), exist_ok=True)\n",
    "        plt.savefig(savepdf, bbox_inches=\"tight\")\n",
    "    \n",
    "\n",
    "def is_valid_attrs(attrs):\n",
    "    # Check if attrs is a list with exactly two items\n",
    "    if not isinstance(attrs, list) or len(attrs) != 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if each item in the list is a tuple with exactly two integers\n",
    "    for item in attrs:\n",
    "        if not isinstance(item, tuple) or len(item) != 2:\n",
    "            return False\n",
    "        if not all(isinstance(i, int) for i in item):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "    \n",
    "        \n",
    "def healthy_r(r):\n",
    "    logs = []\n",
    "\n",
    "    if \"scores\" not in r:\n",
    "        msg = f\"no `scores`\"\n",
    "        logs.append(msg)\n",
    "        return False, logs\n",
    "    \n",
    "    if \"status\" not in r or not r[\"status\"]:\n",
    "        msg = f\"`status == False`\"\n",
    "        logs.append(msg)\n",
    "        return False, logs\n",
    "    \n",
    "    attribute_loc = list(sorted(list(itertools.chain.from_iterable(r[\"attributes_loc\"].values()))))\n",
    "    if not attribute_loc:\n",
    "        msg = f\"no `attribute_loc`\"\n",
    "        logs.append(msg)\n",
    "        return False, logs\n",
    "\n",
    "    \n",
    "    if np.isnan(r[\"te_log\"]) or np.isinf(r[\"te_log\"]):\n",
    "        msg = f\"no `te_log`\"\n",
    "        logs.append(msg)\n",
    "        return False, logs\n",
    "\n",
    "    \n",
    "    if np.isnan(r[\"te\"]) or np.isinf(r[\"te\"]):\n",
    "        msg = f\"no `te`\"\n",
    "        logs.append(msg)\n",
    "        return False, logs\n",
    "\n",
    "    return True, logs\n",
    "\n",
    "\n",
    "def find_insert_position(tuples_list, target_tuple):\n",
    "    # Extract the end values of each tuple in the list\n",
    "    ends = [t[1] for t in tuples_list]\n",
    "    \n",
    "    # Find the position using bisect_right\n",
    "    position = bisect.bisect_right(ends, target_tuple[0])\n",
    "    \n",
    "    return position\n",
    "\n",
    "def transform_to_title_case(input_string):\n",
    "    words = input_string.split(\" \")\n",
    "    transformed_string = ' '.join(word.capitalize() for word in words)\n",
    "    return transformed_string\n",
    "\n",
    "def tokens_space_division(data, experiment_type, num_layers=12, do_log=True):\n",
    "    avg_effects = [\n",
    "        \"question_tokens\",\n",
    "\n",
    "        # \"answer\",\n",
    "        # \"answer_token\",\n",
    "        \n",
    "        \"begining_of_context\",\n",
    "\n",
    "        \"first_subject_token\",\n",
    "        \"middle_subject_tokens\",\n",
    "        \"last_subject_token\",\n",
    "\n",
    "        \"context_in_between_tokens\",\n",
    "\n",
    "        \"first_object_token\",\n",
    "        \"middle_object_tokens\",\n",
    "        \"last_object_token\",\n",
    "\n",
    "        \"rest_of_context_tokens\",\n",
    "\n",
    "        \"last_token\",\n",
    "\n",
    "        \"subject_set_tokens\",\n",
    "        \"object_set_tokens\",\n",
    "        \"relation_set_tokens\",\n",
    "    ]\n",
    "    avg_scores = [\"high_score\", \"low_score\", \"te\", \"fixed_score\"]\n",
    "    avg = {name: Avg(size=1, name=name) for name in avg_scores}\n",
    "    avg.update({name: Avg(size=num_layers, name=name) for name in avg_effects})\n",
    "\n",
    "    result = np.array([])\n",
    "    result_std = np.array([])\n",
    "\n",
    "    for r in tqdm(data, total=len(data)):\n",
    "        attribute_locs = list(sorted(list(itertools.chain.from_iterable(r[\"attributes_loc\"].values()))))\n",
    "\n",
    "        attribute_loc = attribute_locs[-1]\n",
    "        start_of_answer = r[\"input_tokens\"].index(\"▁answer\")\n",
    "        start_of_context = r[\"input_tokens\"].index(\"<extra_id_0>\") + 1\n",
    "        start_of_attr, end_of_attr = attribute_loc\n",
    "        end_of_prompt = len(r[\"input_tokens\"])\n",
    "\n",
    "        input_segments = prompt_segmenter([r[\"input_tokens\"]])\n",
    "\n",
    "        object_ranges = find_token_ranges(model.reader_tokenizer, r[\"input_ids\"], r[\"cf\"][0], bounds=input_segments[0][\"context\"])\n",
    "        object_ranges = object_ranges[0] if isinstance(object_ranges, list) and len(object_ranges) > 0 else []\n",
    "\n",
    "        subject_ranges = find_token_ranges(model.reader_tokenizer, r[\"input_ids\"], r[\"prompt\"][\"subj\"], bounds=input_segments[0][\"context\"])\n",
    "        subject_ranges = subject_ranges[0] if isinstance(subject_ranges, list) and len(subject_ranges) > 0 else []\n",
    "\n",
    "        if do_log:\n",
    "            te, ie = r[\"te_log\"], r[\"ie_log\"]\n",
    "        else:\n",
    "            te, ie = r[\"te\"], r[\"ie\"]\n",
    "\n",
    "\n",
    "        subject_pos = object_pos = 0\n",
    "        if experiment_type == \"c\":\n",
    "            try:\n",
    "                subject_pos = find_insert_position(list(sorted(list(itertools.chain.from_iterable(r[\"attributes_loc\"].values())))), subject_ranges)\n",
    "                object_pos = find_insert_position(list(sorted(list(itertools.chain.from_iterable(r[\"attributes_loc\"].values())))), object_ranges)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        \n",
    "        if experiment_type == \"a\":\n",
    "            attrs = [subject_ranges, attribute_loc]\n",
    "        elif experiment_type == \"b\":\n",
    "            attrs = [attribute_loc, object_ranges]\n",
    "        elif experiment_type == \"c\":\n",
    "            attrs = [subject_ranges, object_ranges]\n",
    "\n",
    "        \n",
    "        if not is_valid_attrs(attrs):\n",
    "            continue\n",
    "\n",
    "\n",
    "        avg[\"high_score\"].add(np.array(r[\"cr_ans_score\"]))\n",
    "        avg[\"low_score\"].add(np.array(r[\"crr_score\"]))\n",
    "        avg[\"te\"].add(np.array(te))\n",
    "        avg[\"fixed_score\"].add(ie.max())\n",
    "\n",
    "        avg[\"question_tokens\"].add_all(ie[0:start_of_answer])\n",
    "\n",
    "        # avg[\"answer\"].add_all(ie[start_of_answer:start_of_answer+3])\n",
    "        # avg[\"answer_token\"].add(ie[start_of_answer+3])\n",
    "        avg[\"last_token\"].add(ie[-1])\n",
    "\n",
    "        if experiment_type == \"a\":\n",
    "            attrs = [subject_ranges, attribute_loc]\n",
    "            first_attr = attrs.index(min(attrs))\n",
    "            second_attr = 0 if first_attr == 1 else 1\n",
    "\n",
    "            avg[\"begining_of_context\"].add_all(ie[start_of_context+4:attrs[first_attr][0]])\n",
    "            avg[\"context_in_between_tokens\"].add_all(ie[attrs[first_attr][1]:attrs[second_attr][0]])\n",
    "            avg[\"rest_of_context_tokens\"].add_all(ie[attrs[second_attr][1]:end_of_prompt-1])\n",
    "\n",
    "            avg[\"subject_set_tokens\"].add_all(ie[subject_ranges[0]:subject_ranges[1]])\n",
    "            avg[\"object_set_tokens\"].add_all(ie[attribute_loc[0]:attribute_loc[1]])\n",
    "            \n",
    "            avg[\"relation_set_tokens\"].add_all(ie[start_of_context+4:attrs[first_attr][0]])\n",
    "            avg[\"relation_set_tokens\"].add_all(ie[attrs[first_attr][1]:attrs[second_attr][0]])\n",
    "            avg[\"relation_set_tokens\"].add_all(ie[attrs[second_attr][1]:end_of_prompt-1])\n",
    "\n",
    "            avg[\"first_subject_token\"].add(ie[subject_ranges[0]])\n",
    "            avg[\"middle_subject_tokens\"].add_all(ie[subject_ranges[0]+1:subject_ranges[1]-1])\n",
    "            avg[\"last_subject_token\"].add(ie[subject_ranges[1]-1])\n",
    "\n",
    "            avg[\"first_object_token\"].add(ie[attribute_loc[0]])\n",
    "            avg[\"middle_object_tokens\"].add_all(ie[attribute_loc[0]+1:attribute_loc[1]-1])\n",
    "            avg[\"last_object_token\"].add(ie[attribute_loc[1]-1])\n",
    "\n",
    "        elif experiment_type == \"b\":\n",
    "            attrs = [attribute_loc, object_ranges]\n",
    "            first_attr = attrs.index(min(attrs))\n",
    "            second_attr = 0 if first_attr == 1 else 1\n",
    "\n",
    "            avg[\"begining_of_context\"].add_all(ie[start_of_context+4:attrs[first_attr][0]])\n",
    "            avg[\"context_in_between_tokens\"].add_all(ie[attrs[first_attr][1]:attrs[second_attr][0]])\n",
    "            avg[\"rest_of_context_tokens\"].add_all(ie[attrs[second_attr][1]:end_of_prompt-1])\n",
    "\n",
    "            avg[\"subject_set_tokens\"].add_all(ie[attribute_loc[0]:attribute_loc[1]])\n",
    "            avg[\"object_set_tokens\"].add_all(ie[object_ranges[0]:object_ranges[1]])\n",
    "\n",
    "            avg[\"relation_set_tokens\"].add_all(ie[start_of_context+4:attrs[first_attr][0]])\n",
    "            avg[\"relation_set_tokens\"].add_all(ie[attrs[first_attr][1]:attrs[second_attr][0]])\n",
    "            avg[\"relation_set_tokens\"].add_all(ie[attrs[second_attr][1]:end_of_prompt-1])\n",
    "            \n",
    "            avg[\"first_subject_token\"].add(ie[attribute_loc[0]])\n",
    "            avg[\"middle_subject_tokens\"].add_all(ie[attribute_loc[0]+1:attribute_loc[1]-1])\n",
    "            avg[\"last_subject_token\"].add(ie[attribute_loc[1]])\n",
    "\n",
    "            avg[\"first_object_token\"].add(ie[object_ranges[0]])\n",
    "            avg[\"middle_object_tokens\"].add_all(ie[object_ranges[0]+1:object_ranges[1]-1])\n",
    "            avg[\"last_object_token\"].add(ie[object_ranges[1]-1])\n",
    "        elif experiment_type == \"c\":\n",
    "            attrs = [subject_ranges, object_ranges]\n",
    "            first_attr = attrs.index(min(attrs))\n",
    "            second_attr = 0 if first_attr == 1 else 1\n",
    "            \n",
    "            avg[\"begining_of_context\"].add_all(ie[start_of_context+4:attrs[first_attr][0]])\n",
    "            avg[\"context_in_between_tokens\"].add_all(ie[attrs[first_attr][1]:attrs[second_attr][0]])\n",
    "            avg[\"rest_of_context_tokens\"].add_all(ie[attrs[second_attr][1]:end_of_prompt-1])\n",
    "\n",
    "            avg[\"subject_set_tokens\"].add_all(ie[subject_ranges[0]:subject_ranges[1]])\n",
    "            avg[\"object_set_tokens\"].add_all(ie[object_ranges[0]:object_ranges[1]])\n",
    "\n",
    "            avg[\"relation_set_tokens\"].add_all(ie[start_of_context+4:attrs[first_attr][0]])\n",
    "            avg[\"relation_set_tokens\"].add_all(ie[attrs[first_attr][1]:attrs[second_attr][0]])\n",
    "            avg[\"relation_set_tokens\"].add_all(ie[attrs[second_attr][1]:end_of_prompt-1])\n",
    "\n",
    "            avg[\"first_subject_token\"].add(ie[subject_ranges[0]])\n",
    "            avg[\"middle_subject_tokens\"].add_all(ie[subject_ranges[0]+1:subject_ranges[1]-1])\n",
    "            avg[\"last_subject_token\"].add(ie[subject_ranges[1]-1])\n",
    "\n",
    "            avg[\"first_object_token\"].add(ie[object_ranges[0]])\n",
    "            avg[\"middle_object_tokens\"].add_all(ie[object_ranges[0]+1:object_ranges[1]-1])\n",
    "            avg[\"last_object_token\"].add(ie[object_ranges[1]-1])\n",
    "\n",
    "        \n",
    "        result = [avg[name].avg() for name in avg_effects]\n",
    "        result_std = [avg[name].std() for name in avg_effects]\n",
    "    \n",
    "    print_out = [\n",
    "        {\"METRIC\": \"Average Total Effect\", \"VALUE\": avg[\"te\"].avg()},\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"high_score\": avg[\"high_score\"].avg(),\n",
    "        \"low_score\": avg[\"low_score\"].avg(),\n",
    "        \"labels\": [avg[name].humanize() for name in avg_effects],\n",
    "        \"result\": result,\n",
    "        \"result_std\": result_std,\n",
    "        \"size\": num_layers,\n",
    "        \"print_out\": print_out,\n",
    "        \"avg\": avg,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_parametric_behavior(r, threshold=6):\n",
    "    \n",
    "    if sum([int(r[\"answer\"] in p) for p in r[\"crr_predicted\"][1:]]) >= threshold:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def data_prep(cases_directory, experiment_type, do_log=True, parametric_behavior=False):\n",
    "    \n",
    "    experiments = {\n",
    "        \"ordinary_r\": [],\n",
    "        \"no_attn_r\": [],\n",
    "        \"no_mlp_r\": [],\n",
    "    }\n",
    "    for directory in tqdm(cases_directory, total=len(cases_directory)):\n",
    "        for f in glob.glob(os.path.join(directory, \"*.bin\")):\n",
    "            r = torch.load(f, map_location='cpu')\n",
    "            \n",
    "            if parametric_behavior:\n",
    "                if has_parametric_behavior(r[\"ordinary_r\"]):\n",
    "                    for key in r:\n",
    "                        experiments[key].append(calculate_post_proc(r[key], samples=6, experiment_type=experiment_type))\n",
    "            else:\n",
    "                for key in r:\n",
    "                    experiments[key].append(calculate_post_proc(r[key], samples=6, experiment_type=experiment_type))\n",
    "\n",
    "    data = {key: tokens_space_division(experiments[key], experiment_type=experiment_type, do_log=do_log) for key in experiments}\n",
    "    return data, experiments\n",
    "\n",
    "    \n",
    "def plot_r_impact(ordinary, no_attn, no_mlp, title, token_idx=-1, bar_width=0.2, savepdf=None, show_plots=True):\n",
    "    fig, ax = plt.subplots(1, figsize=(5, 3), dpi=300)\n",
    "    ax.bar(\n",
    "        [i - bar_width for i in range(len(ordinary[token_idx]))],\n",
    "        ordinary[token_idx],\n",
    "        width=bar_width,\n",
    "        color=\"#7261ab\",\n",
    "        label=\"Impact of single state\",\n",
    "    )\n",
    "    ax.bar(\n",
    "        [i for i in range(len(no_attn[token_idx]))],\n",
    "        no_attn[token_idx],\n",
    "        width=bar_width,\n",
    "        color=\"#f3201b\",\n",
    "        label=\"Impact with Attn severed\",\n",
    "    )\n",
    "    ax.bar(\n",
    "        [i + bar_width for i in range(len(no_mlp[token_idx]))],\n",
    "        no_mlp[token_idx],\n",
    "        width=bar_width,\n",
    "        color=\"#20b020\",\n",
    "        label=\"Impact with MLP severed\",\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, pad=30)\n",
    "    ax.set_xticks([i for i in range(0, ordinary.shape[1])])\n",
    "    ax.set_xticklabels(list(range(0, ordinary.shape[1])), rotation=0)\n",
    "    ax.set_ylabel(\"AIE\")\n",
    "    ax.set_xlabel(\"Layers\")\n",
    "\n",
    "    ax.yaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "    # ax.set_ylim(None, max(0, ordinary.max() * 1.5))\n",
    "\n",
    "    # ax.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
    "    ax.legend(\n",
    "        loc='upper center', \n",
    "        bbox_to_anchor=(0.5, 1.19),\n",
    "        ncol=2,\n",
    "        fontsize=8,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    if savepdf:\n",
    "        plt.savefig(os.path.join(savepdf, title.replace(\" \", \"_\").strip().lower() + \".pdf\"), bbox_inches=\"tight\")\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def set_plot_font_size(ax, font_size):\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                 ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:31<00:00,  1.18s/it]\n",
      "100%|██████████| 1246/1246 [00:12<00:00, 99.41it/s]\n",
      "100%|██████████| 1246/1246 [00:12<00:00, 100.03it/s]\n",
      "100%|██████████| 1246/1246 [00:12<00:00, 100.81it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment_type = \"aa\"\n",
    "savepdf = f\"../data/figures/synthetic_context/pse/exp1_object/\"\n",
    "os.makedirs(savepdf, exist_ok=True)\n",
    "\n",
    "cases_directory = [\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital_of/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/color/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/composer/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/country/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/father/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/genre/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/occupation/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/place_of_birth/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/religion/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/sport/cases\",\n",
    "\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P17/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P19/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P20/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P36/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P69/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P106/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P127/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P131/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P159/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P175/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P176/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P276/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P407/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P413/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P495/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P740/cases\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "data, experiments = data_prep(cases_directory, experiment_type=\"a\", parametric_behavior=False)\n",
    "for i, division in enumerate(data[\"ordinary_r\"][\"labels\"]):\n",
    "    r = {key: np.array(data[key][\"result\"]).clip(0, None) for key in data}\n",
    "\n",
    "    if division not in [\"Relation set tokens\", \"Subject set tokens\", \"Object set tokens\"]:\n",
    "        continue\n",
    "    \n",
    "    plot_r_impact(\n",
    "        r[\"ordinary_r\"][i:i+1], \n",
    "        r[\"no_attn_r\"][i:i+1], \n",
    "        r[\"no_mlp_r\"][i:i+1], \n",
    "        title=transform_to_title_case(division), \n",
    "        token_idx=0, \n",
    "        bar_width=0.25, \n",
    "        savepdf=savepdf,\n",
    "        show_plots=False\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:37<00:00,  3.62s/it]\n",
      "100%|██████████| 3726/3726 [02:10<00:00, 28.62it/s]\n",
      "100%|██████████| 3726/3726 [02:08<00:00, 28.93it/s]\n",
      "100%|██████████| 3726/3726 [02:11<00:00, 28.23it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment_type = \"bb\"\n",
    "savepdf = f\"../data/figures/synthetic_context/pse/exp2_subject\"\n",
    "os.makedirs(savepdf, exist_ok=True)\n",
    "\n",
    "cases_directory = [\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital_of/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/color/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/composer/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/country/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/father/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/genre/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/occupation/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/place_of_birth/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/religion/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/sport/cases\",\n",
    "\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P17/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P19/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P20/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P36/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P69/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P106/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P127/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P131/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P159/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P175/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P176/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P276/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P407/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P413/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P495/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P740/cases\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "data, experiments = data_prep(cases_directory, experiment_type=\"b\")\n",
    "for i, division in enumerate(data[\"ordinary_r\"][\"labels\"]):\n",
    "    r = {key: np.array(data[key][\"result\"]).clip(0, None) for key in data}\n",
    "\n",
    "    if division not in [\"Relation set tokens\", \"Subject set tokens\", \"Object set tokens\"]:\n",
    "        continue\n",
    "\n",
    "    plot_r_impact(\n",
    "        r[\"ordinary_r\"][i:i+1], \n",
    "        r[\"no_attn_r\"][i:i+1], \n",
    "        r[\"no_mlp_r\"][i:i+1], \n",
    "        title=f\"{division}\", \n",
    "        token_idx=0, \n",
    "        bar_width=0.25, \n",
    "        savepdf=savepdf,\n",
    "        show_plots=False\n",
    "    )\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:19<00:00,  2.93s/it]\n",
      "100%|██████████| 3726/3726 [02:16<00:00, 27.21it/s]\n",
      "100%|██████████| 3726/3726 [02:12<00:00, 28.12it/s]\n",
      "100%|██████████| 3726/3726 [02:06<00:00, 29.36it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment_type = \"cc\"\n",
    "savepdf = f\"../data/figures/synthetic_context/pse/exp2_relation\"\n",
    "os.makedirs(savepdf, exist_ok=True)\n",
    "\n",
    "cases_directory = [\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital_of/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/color/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/composer/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/country/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/father/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/genre/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/occupation/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/place_of_birth/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/religion/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/sport/cases\",\n",
    "\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P17/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P19/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P20/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P36/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P69/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P106/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P127/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P131/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P159/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P175/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P176/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P276/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P407/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P413/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P495/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P740/cases\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "data, experiments = data_prep(cases_directory, experiment_type=\"c\")\n",
    "for i, division in enumerate(data[\"ordinary_r\"][\"labels\"]):\n",
    "    r = {key: np.array(data[key][\"result\"]).clip(0, None) for key in data}\n",
    "\n",
    "    if division not in [\"Relation set tokens\", \"Subject set tokens\", \"Object set tokens\"]:\n",
    "        continue\n",
    "\n",
    "    plot_r_impact(\n",
    "        r[\"ordinary_r\"][i:i+1], \n",
    "        r[\"no_attn_r\"][i:i+1], \n",
    "        r[\"no_mlp_r\"][i:i+1], \n",
    "        title=f\"{division}\", \n",
    "        token_idx=0, \n",
    "        bar_width=0.25, \n",
    "        savepdf=savepdf,\n",
    "        show_plots=False\n",
    "    )\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_specific_effects_per_relation(cases_directories, experiment_type, savepdf=None):\n",
    "\n",
    "    for cases_directory in cases_directories:\n",
    "        if savepdf:\n",
    "            _savepdf = os.path.join(savepdf, cases_directory.split(\"/\")[-2])\n",
    "            os.makedirs(_savepdf, exist_ok=True)\n",
    "        else:\n",
    "            _savepdf = None\n",
    "\n",
    "        data, experiments = data_prep([cases_directory], experiment_type=experiment_type)\n",
    "        for i, division in enumerate(data[\"ordinary_r\"][\"labels\"]):\n",
    "            r = {key: np.array(data[key][\"result\"]).clip(0, None) for key in data}\n",
    "\n",
    "            if division not in [\"Relation set tokens\", \"Subject set tokens\", \"Object set tokens\"]:\n",
    "                continue\n",
    "\n",
    "            plot_r_impact(\n",
    "                r[\"ordinary_r\"][i:i+1], \n",
    "                r[\"no_attn_r\"][i:i+1], \n",
    "                r[\"no_mlp_r\"][i:i+1], \n",
    "                title=f\"{division}\", \n",
    "                token_idx=0, \n",
    "                bar_width=0.25, \n",
    "                savepdf=_savepdf, \n",
    "                show_plots=False,\n",
    "            )\n",
    "            \n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 681.08it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 687.21it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 678.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.07it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 1201.14it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 1212.27it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 1218.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.16it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1210.04it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1336.40it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1355.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.14it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1211.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1299.25it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1308.47it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "100%|██████████| 101/101 [00:00<00:00, 643.23it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 649.66it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 644.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.18it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 1101.93it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 1217.03it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 1215.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 1116.83it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 1201.93it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 1144.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.51it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1123.73it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1181.83it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1183.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 1131.88it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 1162.67it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 1131.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 1167.72it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 1207.85it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 1207.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 1112.06it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 1126.06it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 1099.90it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "100%|██████████| 101/101 [00:00<00:00, 669.36it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 685.33it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 673.99it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "100%|██████████| 101/101 [00:00<00:00, 627.49it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 637.44it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 631.20it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "100%|██████████| 101/101 [00:00<00:00, 716.15it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 714.02it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 719.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 674.56it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 683.05it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 681.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1127.52it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1149.83it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1154.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.73it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 1172.27it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 1197.83it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 1197.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1076.38it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1092.48it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1085.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.21it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 1205.06it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 1227.69it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 1208.04it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "100%|██████████| 101/101 [00:00<00:00, 644.85it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 646.04it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 641.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1137.05it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1152.88it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1154.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 66/66 [00:00<00:00, 775.50it/s]\n",
      "100%|██████████| 66/66 [00:00<00:00, 775.69it/s]\n",
      "100%|██████████| 66/66 [00:00<00:00, 772.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 1048.08it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 1085.82it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 1085.29it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.71s/it]\n",
      "100%|██████████| 101/101 [00:00<00:00, 719.10it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 710.78it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 735.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.76it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 1197.08it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 1236.45it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 1262.15it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "100%|██████████| 101/101 [00:00<00:00, 688.52it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 681.70it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 699.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 898.39it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 886.90it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 890.94it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment_type = \"aa\"\n",
    "cases_directory_relations = [\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital_of/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/color/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/composer/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/country/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/father/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/genre/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/occupation/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/place_of_birth/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/religion/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/sport/cases\",\n",
    "\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P17/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P19/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P20/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P36/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P69/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P106/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P127/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P131/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P159/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P175/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P176/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P276/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P407/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P413/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P495/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P740/cases\",\n",
    "]\n",
    "savepdf = f\"../data/figures/synthetic_context/pse/exp1_object_per_relation\"\n",
    "os.makedirs(savepdf, exist_ok=True)\n",
    "\n",
    "path_specific_effects_per_relation(cases_directory_relations, experiment_type=\"a\", savepdf=savepdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.04s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 361.86it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 361.63it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 366.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 78/78 [00:00<00:00, 861.03it/s]\n",
      "100%|██████████| 78/78 [00:00<00:00, 874.12it/s]\n",
      "100%|██████████| 78/78 [00:00<00:00, 857.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.19it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1344.76it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1279.47it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1297.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1222.06it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1245.65it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1256.09it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.31s/it]\n",
      "100%|██████████| 302/302 [00:00<00:00, 327.56it/s]\n",
      "100%|██████████| 302/302 [00:00<00:00, 331.79it/s]\n",
      "100%|██████████| 302/302 [00:00<00:00, 331.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.35it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 1193.11it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 1241.61it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 1226.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 899.01it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 915.38it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 896.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.35it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1153.34it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1161.03it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1138.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 958.44it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 963.44it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 968.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 1023.76it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 1052.13it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 1022.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 792.73it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 803.32it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 826.44it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.62s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 343.59it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 350.15it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 350.20it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.57s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 323.69it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 320.36it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 318.77it/s]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.42s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 371.45it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 370.52it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 365.33it/s]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.11s/it]\n",
      "100%|██████████| 249/249 [00:00<00:00, 373.93it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 368.93it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 371.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 909.39it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 914.76it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 917.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 993.60it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 941.19it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1007.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 787.03it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 817.88it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 815.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1010.44it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1025.90it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1004.46it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.40s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 332.07it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 330.16it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 325.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 1105.28it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 1121.18it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 1129.85it/s]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.59s/it]\n",
      "100%|██████████| 198/198 [00:00<00:00, 469.80it/s]\n",
      "100%|██████████| 198/198 [00:00<00:00, 463.54it/s]\n",
      "100%|██████████| 198/198 [00:00<00:00, 469.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "100%|██████████| 75/75 [00:00<00:00, 757.24it/s]\n",
      "100%|██████████| 75/75 [00:00<00:00, 763.37it/s]\n",
      "100%|██████████| 75/75 [00:00<00:00, 761.63it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.70s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 366.65it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 362.56it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 365.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1031.95it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1026.23it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1036.00it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.64s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 348.77it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 357.31it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 349.44it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n",
      "100%|██████████| 180/180 [00:00<00:00, 521.72it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 528.68it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 528.78it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment_type = \"bb\"\n",
    "cases_directory_relations = [\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital_of/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/color/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/composer/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/country/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/father/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/genre/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/occupation/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/place_of_birth/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/religion/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/sport/cases\",\n",
    "\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P17/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P19/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P20/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P36/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P69/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P106/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P127/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P131/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P159/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P175/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P176/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P276/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P407/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P413/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P495/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P740/cases\",\n",
    "]\n",
    "savepdf = f\"../data/figures/synthetic_context/pse/exp2_subject_per_relation\"\n",
    "os.makedirs(savepdf, exist_ok=True)\n",
    "\n",
    "path_specific_effects_per_relation(cases_directory_relations, experiment_type=\"b\", savepdf=savepdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.01s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 362.59it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 362.45it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 356.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "100%|██████████| 78/78 [00:00<00:00, 884.30it/s]\n",
      "100%|██████████| 78/78 [00:00<00:00, 896.67it/s]\n",
      "100%|██████████| 78/78 [00:00<00:00, 901.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1271.74it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1285.94it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1304.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.65it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1220.61it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1244.82it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1265.41it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.22s/it]\n",
      "100%|██████████| 302/302 [00:00<00:00, 339.36it/s]\n",
      "100%|██████████| 302/302 [00:00<00:00, 352.10it/s]\n",
      "100%|██████████| 302/302 [00:00<00:00, 343.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.22it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 1187.96it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 1236.65it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 1234.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 920.80it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 934.01it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 917.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1166.03it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1235.28it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1176.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 967.88it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 898.78it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 1000.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 1030.50it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 1031.61it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 1028.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 822.86it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 805.90it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 810.70it/s]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.72s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 346.55it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 350.66it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 347.17it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.75s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 326.11it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 333.74it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 329.37it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.23s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 377.28it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 376.52it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 362.24it/s]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.39s/it]\n",
      "100%|██████████| 249/249 [00:00<00:00, 357.96it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 363.78it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 371.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 917.87it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 923.83it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 913.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 973.44it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 981.33it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 976.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 801.44it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 802.18it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 794.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 983.84it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1006.10it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1003.32it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.49s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 341.99it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 336.69it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 339.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 1109.56it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 1163.52it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 1151.46it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.38s/it]\n",
      "100%|██████████| 198/198 [00:00<00:00, 485.28it/s]\n",
      "100%|██████████| 198/198 [00:00<00:00, 490.17it/s]\n",
      "100%|██████████| 198/198 [00:00<00:00, 484.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "100%|██████████| 75/75 [00:00<00:00, 768.93it/s]\n",
      "100%|██████████| 75/75 [00:00<00:00, 769.38it/s]\n",
      "100%|██████████| 75/75 [00:00<00:00, 796.76it/s]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.43s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 377.15it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 377.24it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 387.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1034.88it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1035.85it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 1040.46it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.88s/it]\n",
      "100%|██████████| 303/303 [00:00<00:00, 352.02it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 354.24it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 360.92it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n",
      "100%|██████████| 180/180 [00:00<00:00, 531.35it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 545.34it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 555.52it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment_type = \"cc\"\n",
    "cases_directory_relations = [\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/capital_of/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/color/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/composer/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/country/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/father/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/genre/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/occupation/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/place_of_birth/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/religion/cases\",\n",
    "    f\"../experiments/ct/popqa/{experiment_type}/matched-both-repr/sport/cases\",\n",
    "\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P17/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P19/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P20/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P36/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P69/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P106/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P127/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P131/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P159/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P175/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P176/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P276/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P407/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P413/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P495/cases\",\n",
    "    f\"../experiments/ct/peq/{experiment_type}/matched-both-repr/P740/cases\",\n",
    "]\n",
    "savepdf = f\"../data/figures/synthetic_context/pse/exp2_relation_per_relation\"\n",
    "os.makedirs(savepdf, exist_ok=True)\n",
    "\n",
    "path_specific_effects_per_relation(cases_directory_relations, experiment_type=\"c\", savepdf=savepdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c924a3b366f2428512e8119f66ee34e750cd77838d106b793d81686518d5af6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
